---
title: "Movie Recommendation System"
author: "Lawrence Ejike"
date: "10/05/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1. Introduction

### 1.1. Background
Since the emergence of Netflix and in more recent years, there has been an increase in the number of online movie streaming platforms. Some examples include Amazon Prime, Disney Plus, Crave, and Apple TV, to name a few. A platform's ability to keep users engaged means more time spent by the users on the platform, which translates to longer periods of subscription by the user and more revenue for the platform providers. Thus, significant investments have been made into recommendation systems by companies that own and operate these platforms, as epitomized by the Netflix Prize competition of 2007-2009 that had a grand prize of US$1,000,000. Recommendation systems use ratings that users have given items to make a specific recommendation [1].

### 1.2. Executive summary
This report details the development of a movie recommendation system. The project  utilized MovieLens 10M data set that was generated by GroupLens Research Lab. It contains over 10 million ratings for 10681 movies by 71567 users of the online movie recommender services, MovieLens.  

The primary goal of this project was to create a movie recommendation system, one that uses ratings that users have given to movies to make specific predictions of the rating they would give other movies, thereby severing as a recommendation. The secondary goal of the project was to obtain root mean squared error, RMSE (a measure of the deviation of predicted values from observed values) that is less than or equal to 0.86490. 

The development of the recommendation system was achieved using machine learning as well as other data science tools. The Movielens 10M data set was first split into two sets, validation and edx set, and the latter set was split into a train and test set. Predictive models using the train set were made that incorporate the effect of various features (movieId, userId, time/date, and genre) and these models were tested on the test set. The performance of each model was evaluated against each other and the target RMSE.  Regularization was required in later models to offset the distortion arising from the difference in the number of ratings for each unique feature entry, thereby improving the performance of the models. After the target RMSE was obtained on the test set, the corresponding model was applied to the edx set and used to predict the ratings in the validation set, yielding a final RMSE value that met the target.

## 2. Method/Analysis

The following packages, tidyverse, caret, data.table, and lubridate were installed and used to download, clean, and create the data set.

i) Tidyverse was used for data wrangling, web-scrapping, joining, and reshaping data tables.
ii) Caret package was used for machine learning and building prediction algorithms. 
iii) The data.table package was used as a fast file reader
iv) Lubridate package was used to simplify work with dates and time
```{r,  echo=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
```

### 2.1 Data Cleaning
#### 2.1.1.	Creating training (edx) set and hold out test (validation) set
•	The 10M MovieLens Data was downloaded as a ".DAT" file from the web and the rating data file was converted into a dataframe, named ratings. The four columns (variables) of the dataframe are userId for each user's unique identification, movieId representing each movie's unique ID number, rating for the score given to a movie by a user, and timestamp denoting the time of movie rating.
```{r,  echo=FALSE, message=FALSE}
dl <- tempfile() #creates a temporary file name
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl) #download file from web to temp file

#read rating data file into a data table with assigned column names
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp")) 
```

•	10M MovieLens data was also unpacked into a matrix called 'movies', and comprised of three columns named as 
1)'movieId' representing each movie's unique ID number, 
2)'title' representing each movie's title, and 
3)'genres' representing each movie's genre type(s).
```{r,  echo=FALSE}
# extract movies (string) file into a matrix with 3 column and assign column names
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres") 
```

•	The matrix, movies was converted to a dataframe (movies) and the components of the dataframe column movieId were converted to  numeric, while those of columns, title, and genres were converted to character.
```{r,  echo=FALSE}
# for R 4.0 or later. convert movie matrix to a dataframe
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
```

•	A movielens dataframe was then created by joining the ratings and movies dataframes using the shared movieId column with a join function (left_join) that retained userId, movieId, rating, timestamp, title, and genre.
```{r,  echo=FALSE}
# create movielens data set from rating and movie dataframes
movielens <- left_join(ratings, movies, by = "movieId")
```

•	The movielens data was then split using createDataPartition into edx sex (90% of the data) and validation set (remaining 10%). The validation set was updated using semi-join to ensure that userId and movieId it contained were also present in the edx set.
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Split Movielens into edx and validation set. Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
```

•	The rows removed from the validation set were added back into the edx set, hence no data row in left out. The edx set was used as the training set while the validation set was held out for final predictions after the development of the final model. 
```{r,  echo=FALSE, message=FALSE}
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# removes files and table that are no longer needed
rm(dl, ratings, movies, test_index, temp, movielens, removed) 
```
#### 2.1.2.	Creating train set and test set from edx
To be able to test the performance of the models being developed, the edx dataset was split into a train_set and a test_set. The train set was used for model building while the test set was used for prediction and performance checks. The method for creating the train and test set from the edx data set was similar to that for creating the edx and validation set from the original Movielens dataset. 

The train and test sets were created using the createDataPartition function in the caret package. A temporary test set (test_set_pre) was created initially and then modified into a working test set (test_set). The latter was made by removing user and movie entries in the temporary test set that were not present in the train set.
```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1, sample.kind ="Rounding")

# create train set and (temporary) test set.Test set will be 20% of edx data.
test_index <- createDataPartition(edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set_pre <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test_set <- test_set_pre %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

The rows removed from the test set were added back into the train set, hence no data row in left out.
```{r,  echo=FALSE, message=FALSE}
# Add rows removed from test set back into train set
removed <- anti_join(test_set_pre, test_set)
train_set <- rbind(train_set, removed)

rm(test_set_pre, removed, test_index) # remove these dataset
```

The metric used to assess the performance of each model to be built is  residual mean squared error, RMSE. This represents the difference between values predicted by each model and the values observed. In related terms, it is the error made when predicting a movie rating. Hence, the lower the RMSE, the better the predictive power of a given model. The target RMSE for this project is equal to or less than 0.86490. The RMSE function used to assess each model is defined below. The number of digits to be printed was also set with the option function as shown below
```{r }
# define function for computing RMSE for vectors of rating and corresponding predictors
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
# set number of digits to be printed
options(digits = 5)  
```
### 2.2. Data Exploration, Data Visualization, and Modeling
 In this section, I explored the data in the data sets (edx and the train set) using tables and graphs where applicable and gained useful insights that formed the basis of subsequent modeling approaches.
 An examination of the edx data set shows it contains 9000055 rows  and 6 columns or features (userId, movieId, rating, timestamp, title, and genres). As shown in Table 1 below, each row represented a unique combination of features. Since movieId and title represent the same information i.e. a given movie identity, I ignored the feature, 'title' so as not to duplicate information. The feature 'rating' represents the value for which predictions were to be made, and as such, were left out of consideration in the modeling approaches. Thus, the four features that were considered for this recommendation system  were movieId, userId, timestamp, and genre. These features were explored sequentially, in the models presented below. 
```{r,  echo=FALSE, message=FALSE}

edx %>% head() %>% knitr::kable(caption = "Table 1: First 6 rows of the dataset, edx ") # print first 6 entries of edx

# plot histogram of the count of movie ratings
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 20, color = "black") + 
  scale_x_log10() + 
  ggtitle("Figure 1: Distribution of count of rated movies ") +
  xlab("count of movie rating") +
  ylab("frequency")

# plot histogram (frequency distribution) of the count of user ratings
edx %>%
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 20, color = "black") + 
  scale_x_log10() + 
  ggtitle("Figure 2: Distribution of counts of user ratings ") +
  xlab("count of ratings by users") +
  ylab("frequency")
```
  
  Also, Figure 1 shows that some movies are rated more than others. Similarly, Figure 2 shows that some users rate more movies than other users. These differences in the number of ratings for unique features are accounted for in later models with regularization.
  
#### 2.2.1. Model 0: Using the average
Without deep introspection, the simplest model for predicting movie rating is one that gives the average of all movies as an estimate of a movie rating. This model assumes the same rating for all movies and users with the difference being explained by random error and is be expressed as 
rating = average rating + error or symbolically as Y_hat = mu + e.  The estimated average rating, denoted here as mu was obtained as
```{r,  echo=FALSE, message=FALSE}
# compute the average of all the ratings, mu_hat
mu_hat <- mean(train_set$rating)

# for simplicity, use mu for mu_hat for the rest of code
mu <- mu_hat
```
 The RMSE function was applied with mu as the predicted rating for all movie and user combinations. A table, rmse_results was also set up to which all models developed and their corresponding rmse values are stored for easy comparison. 
```{r, echo=FALSE, message=FALSE}
# RMSE obtained by predicting mu for all unknown rating 
Model_0_rmse <- RMSE(test_set$rating, mu)

# set up table for storing various approaches and corresponding RMSE
RMSE_results <- data.frame(Approach = "Using the average", RMSE = Model_0_rmse)

```

#### 2.2.2. Model 1 : Accounting for movie effects
 All movies are not rated the same. For example, some movies are blockbusters while others are not and consequently have lower ratings. The differences could stem from things like a particular movie cast, director, video quality, or particular fad or movie plot. This ultimately creates a bias with some movies being rated higher than others. Here, movie bias was introduced into the previous model and denoted as b_i, the average movie bias for movie i. The rating equation became rating = mu + b_i + e.
  Since there are millions of movies and obtaining the least square estimate of bi using lm() function is not practical, the b_i for each movie was computed as the mean of the differences between each movies rating and the average, mu (b_i = mean (rating- mu)) [1]. The b_i were computed using the train_set and predictions using the model, rating = mu + b_i were made on the test_set.
```{r,  echo=FALSE, message=FALSE}
# Model_1: accounting for movie effect(b_i) 
# create a table of movieId and average bias for each movie
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu)) 

# predict rating as average rating plus movie bias
predicted_ratings <- mu + test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)   

#calculate and assign RMSE to Model 1
Model_1_rmse <- RMSE(predicted_ratings, test_set$rating) 

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Movie Effect Model",
                                     RMSE = Model_1_rmse )) 
```
  
#### 2.2.3. Model 2 : Accounting for user effects
 The average rating by each user was computed and the variability  across users is shown by the histogram in figure 3. 
 
```{r,  echo=FALSE, message=FALSE}
# plot count vs average rating for user that rated over 100 movies
train_set %>%
  group_by(userId) %>%
  summarize(user_mean_rating = mean(rating)) %>%
  filter(n()>=100) %>%
  ggplot(aes(user_mean_rating)) +
  geom_histogram(bins = 35, color = "black") + 
  ggtitle("Figure 3: Distribution of average user ratings ")
```
  
  The user-specific effect or user bias, b_u was then factored into the previous Model 1. As done earlier with movie bias, b_u was computed as follows: b_u = mean (rating- mu - b_i). The rating equation was updated to rating = mu + b_i + b_u + e, thereby incorporating both the movie and user bias.
 
```{r,  echo=FALSE, message=FALSE}
# create a table of userId and average bias for each user
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# predict rating as sum of average rating, movie bias, and user bias
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

#calculate and assign RMSE to Model 2
Model_2_rmse <- RMSE(predicted_ratings, test_set$rating)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Movie + User Effects Model",  
                                     RMSE = Model_2_rmse ))
```
  
#### 2.2.4. Model_3: account for time effect(b_t) ####
 The timestamp column was first converted to a date column using lubridate package. The date could either be grouped by weeks, months, or years. As an example, a smooth plot of rating vs time(date) is shown in Figure 4 and this depicts the variability of rating, albeit marginally, across time.
The time-specific effect or time bias, b_t was then factored into the previous model. The b_t was computed on the train_set as: b_t = mean (rating- mu - b_i - b_u), while predictions were made using rating = mu + b_i + b_u + b_t  on the test_set. Three different models, 3a, 3b, and 3c were built to evaluate the effect of time bias in week, month, and year respectively and then, the best model was selected for further improvement.

```{r,  echo=FALSE, message=FALSE}
#### Model_3a: account for time effect (b_t) in WEEKS ####
# plot average rating vs date (in week) of rating
train_set %>% 
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle( "Figure 4: Effect of time (in weeks) on rating")

# create a table of date (in week) and average time (date) bias 
week_avgs <- train_set %>% 
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(date) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

# predict rating as sum of average rating, movie bias, user bias, and time (week) bias
predicted_ratings <- test_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(week_avgs, by='date') %>%
  mutate(pred = mu + b_i + b_u + b_t) %>%
  .$pred

#calculate and assign RMSE to Model 3a
Model_3a_rmse <- RMSE(predicted_ratings, test_set$rating)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Movie + User + time (week) Effects Model",  
                                     RMSE = Model_3a_rmse ))  



#### Model_3b: account for time effect (b_t) in MONTHS ####
# create a table of date(in month) and average time (date) bias
month_avgs <- train_set %>% 
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "month")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(date) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

# predict rating as sum of average rating, movie bias, user bias, and time (month) bias 
predicted_ratings <- test_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "month")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(month_avgs, by='date') %>%
  mutate(pred = mu + b_i + b_u + b_t) %>%
  .$pred

#calculate and assign RMSE to Model 3b
Model_3b_rmse <- RMSE(predicted_ratings, test_set$rating)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Movie + User + time (month) Effects Model",  
                                     RMSE = Model_3b_rmse ))  



#### Model_3c: account for time effect (b_t) in YEAR ####
# create a table of date(in year) and average time (date) bias
year_avgs <- train_set %>% 
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "year")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(date) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

# predict rating as sum of average rating, movie bias, user bias, and time (year) bias 
predicted_ratings <- test_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "year")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(year_avgs, by='date') %>%
  mutate(pred = mu + b_i + b_u + b_t) %>%
  .$pred

#calculate and assign RMSE to Model 3c
Model_3c_rmse <- RMSE(predicted_ratings, test_set$rating)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Movie + User + time (year) Effects Model",  
                                     RMSE = Model_3c_rmse ))  

```

#### 2.2.5. Model_4: account for multi-genre effect(b_mg) ####
Movies belong to one or more genres. Table 2 below shows the number of unique genre present in the data, as well as the number of times it appears in the genre column and the average rating for the genre. Figure 5 is a scatterplot of rating vs genre, and it shows, as expected, that there are differences in genre rating. Some genres are just liked more than others and generally receive a comparatively higher rating. This bias due to genre contributes either positively or negatively to a movie's rating. 

```{r,  echo=FALSE, message=FALSE}
# create table of unique genre, counts and average rating for thetrain set
genre_rating <- train_set %>% 
  separate_rows(genres, sep = "\\|") %>% 
  group_by(genres) %>% 
  summarize(count = n(),rating = mean(rating)) 

# Print table with title
knitr::kable(genre_rating, caption = "Table 2: Average rating and count of unique genres in the train set")

# plot of genre vs average genre rating for train set
genre_rating %>% 
  ggplot(aes(genres, rating)) + 
  geom_point() + 
  geom_smooth() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Figure 5: Plot of genre vs average genre rating for train_set")
```

For movies belonging to several genres, the total or multi-genre bias can be accounted for by summation of the individual bias of each constituent genre. Alternatively, the multi-genre bias can be obtained as the average rating of each unique genre combination as show in Table 3 below 

```{r,  echo=FALSE, message=FALSE}
# count and average rating for multi-genres
train_set %>% group_by(genres) %>% 
  summarise(count = n(),rating = mean(rating)) %>% 
  top_n(20, count) %>% 
  arrange(desc(count)) %>% 
  knitr::kable(caption = " Table 3: Average rating and count of multi-genre in the train set")
```

With the inclusion of multi-genre effect, the model rating equation became rating = mu + b_i + b_u + b_t + b_mg + e. The b_mg was computed on the train_set as: b_t = mean (rating- mu - b_i - b_u - b_t), while predictions were made using rating = mu + b_i + b_u + b_t + b_mg  on the test_set.

```{r,  echo=FALSE, message=FALSE}
#### Model_4: account for multi-genre effect(b_mg) ####
# create a table of multi-genre and average multi-genre bias, b_mg
genre_avgs <- train_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(week_avgs, by='date') %>%
  group_by(genres) %>%
  summarize(b_mg = mean(rating - mu - b_i - b_u - b_t))

#predict rating as sum of average rating, movie bias, user bias, time (week) bias, and multi-genre bias
predicted_ratings <- test_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(week_avgs, by='date') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu + b_i + b_u + b_t + b_mg) %>%
  .$pred

#calculate and assign RMSE to Model 4
Model_4_rmse <- RMSE(predicted_ratings, test_set$rating)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Movie + User + time +  multi-genre Effects Model",  
                                     RMSE = Model_4_rmse ))

```

#### 2.2.6. Regularization
```{r,  echo=FALSE, message=FALSE}
# Movie bias and number of rating for the top 10 best movies 
train_set %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  arrange(desc(b_i)) %>% 
  select(movieId, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable(caption = "Table 4: Movie bias and number of rating for the top 10 best movies" )

# Movie bias and number of rating for the top 10 worst movies
train_set %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  arrange(b_i) %>% 
  select(movieId, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable(caption = "Table 5: Movie bias and number of rating for the top 10 worst movies")
```

Some movies get rated more than others. As seen in tables 4 & 5 above, when we consider the best or worse rated  movies, we see that these movies were rated by very few users. This small number of ratings results in greater uncertainty and larger estimates of the movie bias, whether positive or negative. We can minimize the distortion arising from such small sample sizes by applying regularization, which penalizes large estimates formed using small sample sizes [1]. In applying regularization, a regularized bias for a feature like movieId  is obtained by dividing the calculated bias by a sum of the number of rating (n) for the movie i and a correction factor, lambda. This shrinks the bias of movies with a small number of ratings toward zero and leaves those with a large number of ratings unaffected. Lambda for each feature (movieId, userId, time, genre) bias was optimized over a range of values(0 to 10) to find which gives the minimum RMSE.

#### 2.2.7. Model 5: Regularized movie effect model
 The optimized lambda value for movie bias was obtained by testing out different numbers in the range from 0 to 10 at 0.25 increments with the use of the sapply function. The lambda that gave the minimum value of RMSE in the optimization expression was chosen as the optimized lambda, lambda_i. This value, plugged back into the bias calculations, followed by subsequent prediction also gave the same RMSE value as the minimum RMSE as expected. 
 
```{r,  echo=FALSE, message=FALSE}
#### Model 5: Regularized movie effect model ####
#set range of value for lambda
lambdas <- seq(0, 10, 0.25)

#optimize lambda for movie
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  movie_reg_avgs <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  predicted_ratings <- test_set %>%
    left_join(movie_reg_avgs, by = "movieId") %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})

#plot lambdas vs rmses
qplot(lambdas, rmses) + 
  ggtitle("Fig 6: Optimization of lambda for movie bias ")

# assign lambda_i to the lambda that gives minimum rmse
lambda_i <- lambdas[which.min(rmses)]

#A Use l to denote lambda. Assign l to 2.5 (the value obtained for lambda_i)
l <- 2.5
#B set average rating as mu
mu <- mean(train_set$rating)
#C create a table of movieId and regularized average bias(b_i) for each movie
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+l))
#D predict rating as average rating(mu) plus regularized movie bias(b_i)
predicted_ratings <- test_set %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i ) %>%
  .$pred
#E calculate and assign RMSE to Model 5
Model_5_rmse <- RMSE(predicted_ratings, test_set$rating)

# Note: The set of codes in the previous 5 comments (#A,#B,#C,#D & #E) give the same
# value as the min RMSE obtained from optimization. These steps will be bypassed hereafter
# by simply assigning min RMSE as the RMSE of the model being considered

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Regularized Movie Effects Model",  
                                     RMSE = Model_5_rmse ))

```
 
#### 2.2.8. Model 6: Regularized movie + user effect model
 For Model 6, the user bias was regularized since some users rate more than others. With lambda_i fixed, the optimal lambda for user bias was obtained using the sapply function over the range 0 to 10 and saved as lambda_u. The minimum RMSE was assigned as the RMSE for Model 6.
 
```{r,  echo=FALSE, message=FALSE}
#### Model 6: Regularized movie + user effect model ####
#set range of value for lambda
lambdas <- seq(0, 10, 0.25)

#optimize lambda for users with lambda_i = 2.5
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  movie_reg_avgs <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n() + 2.5))
  user_reg_avgs <- train_set %>% 
    left_join(movie_reg_avgs, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- test_set %>% 
    left_join(movie_reg_avgs, by = "movieId") %>%
    left_join(user_reg_avgs, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})

#plot lambdas vs rmses
qplot(lambdas, rmses) +
  ggtitle("Fig 7: Optimization of lambda for user bias ")

# assign lambda to the value that gives minimum rmse
lambda_u <- lambdas[which.min(rmses)]

#assign minimum rmse to Model 6
Model_6_rmse <-min(rmses)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Regularized Movie + User Effects Model",  
                                     RMSE = Model_6_rmse ))

```
 
#### 2.2.9. Model 7: Regularized movie + user + time effect model
 For model 7, the time (in weeks) effect was incorporated into the previous model 6. Using the already obtained values of lambda_i and lambda_u, an optimized lambda for time effect (lambda_t) was obtained using sapply function over range 0 to 10 and  The RMSE for this model corresponded to the minimum RMSE from the optimization
 
```{r,  echo=FALSE, message=FALSE}
#### Model 7: Regularized movie + user + time effect model ####
#set range of value for lambda
lambdas <- seq(0, 10, 0.25)

# Add date column to the datasets outside of 'sapply' and reduce iteration time during optimization
train_set_d <- train_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week"))  
test_set_d <- test_set %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) 

#Assign lambda_i = 2.5, lambda_u = 5, and optimize lambda for time (date) 
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set_d$rating)
  movie_reg_avgs <- train_set_d %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+2.5))
  user_reg_avgs <- train_set_d %>%
    left_join(movie_reg_avgs, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+5))
  time_reg_avgs <- train_set_d %>%
    left_join(movie_reg_avgs, by='movieId') %>%
    left_join(user_reg_avgs, by='userId') %>%
    group_by(date) %>%
    summarize(b_t = sum(rating - mu - b_i - b_u)/(n()+l))
  
  predicted_ratings <- test_set_d %>%
    left_join(movie_reg_avgs, by='movieId') %>%
    left_join(user_reg_avgs, by='userId') %>%
    left_join(time_reg_avgs, by='date') %>%
    mutate(pred = mu + b_i + b_u + b_t) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})

# plot lambdas vs rmses
qplot(lambdas, rmses) +
  ggtitle("Fig 8: Optimization of lambda for time bias ")

# assign lambda_t as lambda that gives the minimum rmse
lambda_t <- lambdas[which.min(rmses)]

#assign minimum rmse to Model 7
Model_7_rmse <-min(rmses)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Regularized Movie + User + Time Effects Model",  
                                     RMSE = Model_7_rmse ))
```
 
#### 2.2.10. Model 8 : Regularized movie + user + time + multi-genre model
Model 8 accounted for all features (movieId, user, time, and genre), much like model 4. For model 8, the multi-genre bias was factored into model 7. For optimization of lambda for multi-genre, all previously obtained lambdas for movies, users, and time were kept constant. Consequently, the RMSE for model 8 corresponded to the RMSE obtained with all optimized lambda values for the features.

```{r,  echo=FALSE, message=FALSE}
#### Model 8: Regularized movie + user + time + multi-genre model ####
#set range of value for lambda
lambdas <- seq(0, 10, 0.25)

# assign lambda_i = 2.5, lambda_u = 5,lambda_t = 2.5 and optimize lambda for multi-genre 
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set_d$rating)
  movie_reg_avgs <- train_set_d %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+2.5))
  user_reg_avgs <- train_set_d %>%
    left_join(movie_reg_avgs, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+5))
  time_reg_avgs <- train_set_d %>%
    left_join(movie_reg_avgs, by='movieId') %>%
    left_join(user_reg_avgs, by='userId') %>%
    group_by(date) %>%
    summarize(b_t = sum(rating - mu - b_i - b_u)/(n()+2.5))
  m_genre_reg_avgs <- train_set_d %>%
    left_join(movie_reg_avgs, by='movieId') %>%
    left_join(user_reg_avgs, by='userId') %>%
    left_join(time_reg_avgs, by='date') %>%
    group_by(genres) %>%
    summarize(b_mg = sum(rating - mu - b_i - b_u - b_t)/(n()+l))
  
  predicted_ratings <- test_set_d %>%
    left_join(movie_reg_avgs, by='movieId') %>%
    left_join(user_reg_avgs, by='userId') %>%
    left_join(time_reg_avgs, by='date') %>%
    left_join(m_genre_reg_avgs, by='genres') %>%
    mutate(pred = mu + b_i + b_u + b_t + b_mg) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})

#plot lambdas vs rmses
qplot(lambdas, rmses) +
  ggtitle("Fig 9: Optimization of lambda for multi-genre bias ")

#assign lambda_mg as lambda that gives minimum rmse
lambda_mg <- lambdas[which.min(rmses)]

#assign minimum rmse to Model 8
Model_8_rmse <-min(rmses)

# Update RMSE table
RMSE_results <- bind_rows(RMSE_results,
                          data.frame(Approach ="Regularized Movie + User + Time + Multi-genre Effects Model",  
                                     RMSE = Model_8_rmse ))
```

#### 2.2.11. Predicting ratings on the validation set
  Model 8 (Regularized movie + user + time + multi-genre model) was then formatted with the edx dataset as a train set and then used to make prediction on the validation set. 
  
```{r,  echo=FALSE, message=FALSE}
#### Prediction on validation set using Model 8 and edx dataset ####

#set average rating on edx as mu
mu <- mean(edx$rating)

# create a table of movieId and regularized average movie bias
movie_avg <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n() + 2.5))

# create a table of userId and regularized average user bias
user_avg <- edx %>%
  left_join(movie_avg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n() + 5))

# create a table of dates and regularized average time bias
time_avg <- edx %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  group_by(date) %>%
  summarize(b_t = sum(rating - mu - b_i - b_u)/(n() + 2.5))

# create a table of genre and regularized average multi-genre bias
m_genre_avg <- edx %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(time_avg, by='date') %>%
  group_by(genres) %>%
  summarize(b_mg = sum(rating - mu - b_i - b_u - b_t)/(n() + 2))

# predict rating as sum of average rating, and regularized values of movie bias, 
# user bias, time(week) bias, and multi-genre bias
predicted_ratings <- validation %>%
  mutate(dates = as_datetime(timestamp), date = round_date(dates, unit = "week")) %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(time_avg, by='date') %>%
  left_join(m_genre_avg, by='genres') %>%
  mutate(pred = mu + b_i + b_u + b_t + b_mg) %>%
  .$pred

# calculate RMSE based of predicted rating. Assign to rmse_val
rmse_val <-RMSE(predicted_ratings, validation$rating)
```

## 3. Result
```{r,  echo=FALSE, message=FALSE}
#format RMSE column to 5 decimal place
RMSE_results$RMSE <- format(RMSE_results$RMSE,digits = 5) 

#print table
RMSE_results %>% knitr::kable(caption = "Table 6: RMSE results for different models")
```

Section 2.2 outlined the different modeling approaches for this movie recommendation system. The results for the models are displayed in the Table 6 above. The base model, Model 0 (Using the Average) predicted the average rating of the train_set as the rating for entry in the test set. This gave an RMSE value of 1.05990, which was far greater than the target RMSE (<= 0.86490). Building on this model, Model 1 (Movie Effect Mode) incorporated the average movie bias and gave an RMSE of 0.94374. This was lower than the 'using the average' model but fell short of the target. 

Model 2 (Movie + User Effects Model) improved on Model 1 by accounting for both movie & user bias, and this was reflected in the lower RMSE value of 0.86593. However, this value still did not meet the target RMSE. Model 3a, 3b, and 3c incorporated the effect of time in weeks, months, and year respectively into Model 2. In other words, in addition to accounting for movie and user bias as in Model 2, Model 3a accounted for the effect of the week of rating; Model 3b accounted for the month (but not week) of rating; while model 3c accounted for the year of the rating but not week or month. The RMSE for Model 3a, 3b and 3c were 0.86584, 0.86591 and 0.86593 respectively. As shown, Model 3a, which uses average time bias over weeks outperformed Model 3b (months) and 3c (years). It also performed better than the previous Model 2. Nevertheless, Model 3a still did not meet the target RMSE. In Model 4, the effect of genre (multi-genre) was accounted for as an addition to Model 3a. This lowered the RMSE value from 0.86584 to 0.86550, yet the latter was still higher than the target RMSE.

Having considered all features, regularization was then applied to offset the distortion present due to the discrepancy in the number of ratings between each unique member of features being considered. In Model 5 (Regularized Movie Effect Model), an optimized lambda of 2.5 was obtained for movie bias (as shown in Figure 6) and subsequently, an RMSE value of 0.94367. This RMSE was better than the non-regularized Model 1 but was far off the target still. For Model 6 (Regularized Movie + User Effects Model), with the lambda_i(movie) set as 2.5, an optimized lambda of 5 was obtained for lambda_u (user) as shown in figure 7. Model 6's prediction of rating on the test_set as indicated by its RMSE value of 0.86526 was much better than Model 5's predictions, as well as those of all preceding models. Notwithstanding, Model 6 did not meet the target RMSE.

Model 7 (Regularized Movie + User + Time Effects Model) introduced a regularization of the time bias (in weeks) as an addition to Model 6. Using previously obtained values of lambda_i and lambda_u, an optimized value of 2.5 (shown in figure 8) was obtained for lambda_t (time) as well as a corresponding RMSE of 0.86514. Although this was the lowest RMSE yet, it was still shy of the target. However, by introducing regularization of multi-genre bias in Model 8 (Regularized Movie + User + Time + Multi-genre Effects Model), I obtained an optimized value of 2 (shown in figure 9) for lambda_mg (multi_genre) and an RMSE of 0.86482  which met and exceeded the target RMSE(<= 0.86490).

Using model 8 updated with the edx set which contained all the movieId and UserId present in the validation set,ratings were predicted on the validation set and an RMSE value of 0.86434 was obtained when compared to the observed ratings. This RMSE value met and exceeded the target RMSE(<= 0.86490) for the project. 


## 4. Conclusion
This report outlines the development of a recommendation system that predicts the rating that unique users give unique movies. The project was built on the Movielens 10M data set which contained over 10 million ratings as well as 10681 movies and 71567 users. A secondary goal for this project was to obtain a target RMSE equal to or less than 0.86490 based on prediction. Initially, several prediction models (Model 1 to 4) were developed that started with the average rating as a base before factoring the effect of movie, user, time, and multi-genre biases sequentially. With these models failing to meet the target RMSE, regularization was applied to the models to give new models (Model 5 to 8) with improved RMSE compared to the corresponding non-regularized models (Model 1 to 4). The best model with the lowest RMSE was a regularized model that accounted for the movie, user, time, and multi-genre biases (Model 8). This model was applied to the edx data set and used to make predictions on the validation set, yielding an RMSE of 0.86434 that met the target RMSE(<= 0.86490) for the project, making the project a success.

One major limitation of the current regularization method employed in this project is that it does not account for variations arising from the fact that (i) some groups of movies have similar rating  patterns and (ii) groups of users have similar rating patterns as well[1]. For the future, improvements on this model should also apply matrix factorization which will account for the aforementioned variations. Matrix factorization would involve converting the data set into a matrix in which each movie gets a column and each user gets a row [1]. Other improvements could also include breaking down multi-genre into unique genres prior to accounting for the genre effect. This will also allow effective factoring of genre for a new movies with new combination of genres than already exist in our data set. 

### Reference
[1] Irizarry R. I., Introduction to Data Science. 2019


